# ðŸ¤– Reinforcement Learning for Generative Models

This section contains works on the use of RL, especially RLHF, to train and align generative models.

- **[Deep Reinforcement Learning from Human Preferences](https://arxiv.org/abs/1706.03741)** â€” Christiano et al., 2017. _Introduced learning from preference comparisons for RL._
- **[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/abs/1909.08593)** â€” Stiennon et al., 2020. _Core work on human feedback for ranking outputs._
- **[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)** â€” Ouyang et al., 2022. _Used RLHF to align GPT models, leading to InstructGPT._
