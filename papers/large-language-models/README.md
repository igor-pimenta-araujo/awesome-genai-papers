# ðŸ“„ Large Language Models (LLMs)

This section contains foundational and state-of-the-art papers related to large-scale language models.

- **[Attention is All You Need](https://arxiv.org/abs/1706.03762)** â€” Vaswani et al., 2017. _Introduced the Transformer architecture, a foundation for modern LLMs._
- **[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)** â€” Devlin et al., 2018. _Demonstrated masked language modeling for effective NLP pretraining._
- **[GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)** â€” Brown et al., 2020. _Showed the capabilities of large-scale autoregressive transformers with few-shot learning._
- **[PaLM: Scaling Language Models with Pathways](https://arxiv.org/abs/2204.02311)** â€” Chowdhery et al., 2022. _Explores training LLMs using the Pathways system and scaling to 540B parameters._
- **[LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)** â€” Touvron et al., 2023. _Open-weight alternatives to proprietary LLMs._
