
# ðŸŽ¥ Multimodal Generative Models

This section focuses on models that combine multiple modalities like text, images, and video.

- **[CLIP: Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)** â€” Radford et al., 2021. _Joint vision-language model trained on large-scale web data._
- **[ALIGN: Scaling up visual and vision-language representation learning with noisy text data](https://arxiv.org/abs/2102.05918)** â€” Jia et al., 2021. _Used contrastive learning to align image and text embeddings._
- **[Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)** â€” Alayrac et al., 2022. _Multimodal model capable of few-shot learning across various visual tasks._
- **[GPT-4 Technical Report](https://openai.com/research/gpt-4)** â€” OpenAI, 2023. _Includes early multimodal capabilities, including image + text input._
